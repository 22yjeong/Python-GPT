{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22yjeong/Python-GPT/blob/main/Python_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URRE2eM88aDn"
      },
      "outputs": [],
      "source": [
        "!pip install datasets torch flask flask-ngrok pyngrok huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37j3_Ju4WSZr"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta-93nef-E7k"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, PeftModel\n",
        "from flask import Flask, request, render_template_string\n",
        "from pyngrok import ngrok, conf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vqQKlzz-I-u"
      },
      "outputs": [],
      "source": [
        "# Set up Quantization arguements\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Use 4-bit quantization for maximum model size reduction\n",
        "    bnb_4bit_compute_dtype=getattr(torch, \"float16\"),  # Use FP16 for computation to balance speed and memory usage\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Use the nf4 quantization type, which is more space-efficient\n",
        ")\n",
        "\n",
        "Auth_token = \"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\"\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('aboonaji/llama2finetune-v2', trust_remote_code = True)\n",
        "model = AutoModelForCausalLM.from_pretrained('aboonaji/llama2finetune-v2', quantization_config = quantization_config, use_auth_token = Auth_token)\n",
        "\n",
        "# Set pad token id and padding side\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "# speed up training\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVBxRfIMYlSM"
      },
      "outputs": [],
      "source": [
        "def update_prompt(dataset_name, new_prompt_template):\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    def modify_prompt(example):\n",
        "        # Replace the prompt in each example with the new prompt template\n",
        "        example['text'] = new_prompt_template.replace('{{ user_query }}', example['text'].split('[/INST]', 1)[-1])\n",
        "        return example\n",
        "\n",
        "    # Apply the modification to the entire dataset\n",
        "    updated_dataset = dataset.map(modify_prompt)\n",
        "\n",
        "    return updated_dataset\n",
        "\n",
        "# New prompt template for the AI assistant\n",
        "new_prompt = (\n",
        "    \"<s>[INST] <<SYS>> You are a helpful AI assistant specialized in the Python coding language.\"\n",
        "    \"Always provide clear, accurate, and concise answers in English.\"\n",
        "    \"If you are unsure about something, it is better to acknowledge your uncertainty than to provide false information.\"\n",
        "    \"Ensure that you understand and can clearly explain how any provided code works.<</SYS>> {{ user_query }} [/INST]\"\n",
        ")\n",
        "#luisroque/instruct-python-llama2-20k luisroque/instruct-python-llama2-500k Megnis/python_code_instructions_18k_LlaMa2\n",
        "# Apply the new prompt engineering to the dataset\n",
        "updated_dataset = update_prompt(\"luisroque/instruct-python-llama2-500k\", new_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tas7Hjafwmqk"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"jtatman/python-code-dataset-500k\")\n",
        "\n",
        "# Define the formatting function\n",
        "def format_for_llama(row):\n",
        "    combined_text = (\n",
        "        f\"<s>[INST] <<SYS>> {row['system']} <</SYS>> {row['instruction']} [/INST] {row['output']} </s>\"\n",
        "    )\n",
        "    return combined_text\n",
        "\n",
        "# Apply the formatting function to each row\n",
        "dataset = dataset.map(lambda x: {\"text\": format_for_llama(x)})\n",
        "\n",
        "# Keep only the new 'text' column\n",
        "dataset = dataset.remove_columns(['system', 'instruction', 'output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbUB18x1-bTP"
      },
      "outputs": [],
      "source": [
        "# Type of optimizer and learning rate scheduler to use during training.\n",
        "optimizer_type = \"paged_adamw_32bit\"\n",
        "\n",
        "# \"cosine\" refers to a cosine annealing schedule, which gradually decreases the learning rate following a cosine curve.\n",
        "scheduler_type = \"cosine\"\n",
        "\n",
        "# Initialize TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Directory where the model checkpoints and logs will be saved.\n",
        "    per_device_train_batch_size=4,  # Batch size to use per GPU/TPU core/CPU during training.\n",
        "    max_steps=200,  # Maximum number of training steps (batches) to perform. Training will stop once this number is reached.\n",
        "    fp16=True,  # Enables mixed precision training using 16-bit floating-point (half-precision) to reduce memory usage and speed up training.\n",
        "    optim=optimizer_type,  # Sets the optimizer to use during training. Here, \"paged_adamw_32bit\" is a memory-efficient variant of AdamW.\n",
        "    lr_scheduler_type=scheduler_type,  # Specifies the learning rate scheduler. \"cosine\" means the learning rate will follow a cosine schedule.\n",
        "    logging_steps=25,  # Log training metrics every 10 steps.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJsu5a-0WHdD"
      },
      "outputs": [],
      "source": [
        "# Initialize the Supervised Fine-Tuning Trainer.\n",
        "trainer = SFTTrainer(\n",
        "    model=model, # Machine model that will be trained\n",
        "    args=training_args, # the training arguments\n",
        "    train_dataset=dataset['train'], # dataset that the Model will be trained on\n",
        "    tokenizer=tokenizer, # tokenizer for the dataset\n",
        "    dataset_text_field = \"text\", #column from th dataset\n",
        "    peft_config = LoraConfig(task_type = \"CAUSAL_LM\", r = 256, lora_alpha = 16, lora_dropout = 0.1) # Parameter Efficient Fine-tuning\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXhFwQdZ_he3"
      },
      "outputs": [],
      "source": [
        "# Resume training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DiW2r6mTUJoe"
      },
      "outputs": [],
      "source": [
        "#NGROK_AUTHTOKEN initialzed\n",
        "NGROK_AUTHTOKEN = '2iqKM5af0TdM3NbjKByb11C3OcB_7VYFTqsQhnAsHTPcbmp6r'\n",
        "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# Initialize the Flask application\n",
        "app = Flask(__name__)\n",
        "\n",
        "# HTML template for the web page\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!doctype html>\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Python GPT</title>\n",
        "        <style>\n",
        "            body {\n",
        "                background-color: #2c2c2c;\n",
        "                color: #ffffff;\n",
        "                font-family: Arial, sans-serif;\n",
        "                margin: 0;\n",
        "                padding: 20px;\n",
        "                text-align: center;\n",
        "            }\n",
        "            h1 {\n",
        "                color: #ffffff;\n",
        "            }\n",
        "            label {\n",
        "                color: #ffffff;\n",
        "            }\n",
        "            input[type=\"text\"] {\n",
        "                width: 60%;\n",
        "                padding: 10px;\n",
        "                margin: 10px 0;\n",
        "                box-sizing: border-box;\n",
        "                border: 2px solid #ffffff;\n",
        "                border-radius: 4px;\n",
        "                background-color: #3c3c3c;\n",
        "                color: #ffffff;\n",
        "                font-size: 16px;  /* Restored original size */\n",
        "            }\n",
        "            input[type=\"submit\"] {\n",
        "                padding: 10px 20px;\n",
        "                border: none;\n",
        "                border-radius: 4px;\n",
        "                background-color: #4caf50;\n",
        "                color: white;\n",
        "                cursor: pointer;\n",
        "            }\n",
        "            input[type=\"submit\"]:hover {\n",
        "                background-color: #45a049;\n",
        "            }\n",
        "            .response-container {\n",
        "                max-height: 400px;\n",
        "                overflow-y: auto;\n",
        "                margin-top: 20px;\n",
        "                border: 1px solid #ffffff;\n",
        "                border-radius: 4px;\n",
        "                background-color: #3c3c3c;\n",
        "                padding: 10px;\n",
        "                text-align: left;\n",
        "            }\n",
        "            .response-container p {\n",
        "                color: #d3d3d3;\n",
        "                background-color: #444444;\n",
        "                padding: 10px;\n",
        "                border-radius: 4px;\n",
        "                display: inline-block;\n",
        "                width: 100%;\n",
        "                box-sizing: border-box;\n",
        "                word-wrap: break-word;\n",
        "                white-space: pre-wrap;\n",
        "            }\n",
        "            .response-container pre {\n",
        "                background-color: #2c2c2c;\n",
        "                color: #ffffff;\n",
        "                padding: 10px;\n",
        "                border-radius: 4px;\n",
        "                overflow-x: auto;\n",
        "            }\n",
        "            .input-group {\n",
        "                margin-bottom: 20px;\n",
        "                display: flex;\n",
        "                justify-content: center;\n",
        "                align-items: center;\n",
        "            }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Python Assistant</h1>\n",
        "        <form action=\"/\" method=\"post\">\n",
        "            <div class=\"input-group\">\n",
        "                <label for=\"query\">Enter your Questions about Python code here</label>\n",
        "            </div>\n",
        "            <div class=\"input-group\">\n",
        "                <input type=\"text\" id=\"query\" name=\"query\" value=\"{{ query }}\" required>\n",
        "            </div>\n",
        "            <input type=\"submit\" value=\"Submit\">\n",
        "        </form>\n",
        "        {% if response %}\n",
        "            <div class=\"response-container\">\n",
        "                <h2>Response:</h2>\n",
        "                <p>{{response|safe}}</p>\n",
        "            </div>\n",
        "        {% endif %}\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model= trainer.model, tokenizer=tokenizer)\n",
        "\n",
        "def format_response(text):\n",
        "    # Replace special tokens and add line breaks for readability\n",
        "    text = text.replace('<s>', '').replace('</s>', '')\n",
        "    text = text.replace('[INST]', '').replace('[/INST]', '')\n",
        "\n",
        "    # Trim leading/trailing whitespace to avoid extra spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Add HTML line breaks\n",
        "    text = text.replace('\\n', '<br>')\n",
        "    return text\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def home():\n",
        "    response = None\n",
        "    if request.method == \"POST\":\n",
        "        user_query = request.form[\"query\"]\n",
        "        max_length = 1024 # Set a fixed max_length value\n",
        "        result = generator(f\"<s>[INST] {user_query} [/INST]\", max_length=max_length)\n",
        "        response = format_response(result[0]['generated_text'])\n",
        "\n",
        "    return render_template_string(HTML_TEMPLATE, response=response)\n",
        "\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:5000\\\"\".format(public_url))\n",
        "\n",
        "    # Run the Flask application\n",
        "    app.run()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "19f9SSMefwmppcmqYuQCyG7MoNREE3zNI",
      "authorship_tag": "ABX9TyNizrtSEGdSZehzfkyRHNZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}